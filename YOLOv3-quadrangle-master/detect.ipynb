{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import argparse\n",
        "import time\n",
        "\n",
        "from models import *\n",
        "from utils.datasets import *\n",
        "from utils.utils import *\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if cuda else 'cpu')\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('-image_folder', type=str, default='data/samples', help='path to images')\n",
        "parser.add_argument('-output_folder', type=str, default='output', help='path to outputs')\n",
        "parser.add_argument('-plot_flag', type=bool, default=True)\n",
        "parser.add_argument('-txt_out', type=bool, default=False)\n",
        "\n",
        "parser.add_argument('-cfg', type=str, default='cfg/yolov3.cfg', help='cfg file path')\n",
        "parser.add_argument('-weights_path', type=str, default='weights/latest.pt', help='weight file path')\n",
        "parser.add_argument('-class_path', type=str, default='data/icdar.names', help='path to class label file')\n",
        "parser.add_argument('-conf_thres', type=float, default=0.2, help='object confidence threshold')\n",
        "parser.add_argument('-nms_thres', type=float, default=0.1, help='iou threshold for non-maximum suppression')\n",
        "parser.add_argument('-batch_size', type=int, default=1, help='size of the batches')\n",
        "parser.add_argument('-img_size', type=int, default=32 * 19, help='size of each image dimension')\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "\n",
        "def detect(opt):\n",
        "\tos.system('rm -rf ' + opt.output_folder)\n",
        "\tos.makedirs(opt.output_folder, exist_ok=True)\n",
        "\n",
        "\t# Load model\n",
        "\tmodel = Darknet(opt.cfg, opt.img_size)\n",
        "\n",
        "\tweights_path = opt.weights_path\n",
        "\tif weights_path.endswith('.weights'):  # saved in darknet format\n",
        "\t\tload_weights(model, weights_path)\n",
        "\telse:  # endswith('.pt'), saved in pytorch format\n",
        "\t\tcheckpoint = torch.load(weights_path, map_location='cpu')\n",
        "\t\tmodel.load_state_dict(checkpoint['model'])\n",
        "\t\tdel checkpoint\n",
        "\n",
        "\tmodel.to(device).eval()\n",
        "\n",
        "\t# Set Dataloader\n",
        "\tclasses = load_classes(opt.class_path)  # Extracts class labels from file\n",
        "\tdataloader = load_images(opt.image_folder, batch_size=opt.batch_size, img_size=opt.img_size)\n",
        "\n",
        "\timgs = []  # Stores image paths\n",
        "\timg_detections = []  # Stores detections for each image index\n",
        "\tprev_time = time.time()\n",
        "\tfor batch_i, (img_paths, img) in enumerate(dataloader):\n",
        "\t\tprint(batch_i, img.shape, end=' ')\n",
        "\n",
        "\t\t# Get detections\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tchip = torch.from_numpy(img).unsqueeze(0).to(device)\n",
        "\t\t\tpred = model(chip)\n",
        "\t\t\tpred = pred[pred[:, :, 8] > opt.conf_thres]\n",
        "\n",
        "\t\t\tif len(pred) > 0:\n",
        "\t\t\t\tdetections = non_max_suppression(pred.unsqueeze(0), 0.1, opt.nms_thres)\n",
        "\t\t\t\timg_detections.extend(detections)\n",
        "\t\t\t\timgs.extend(img_paths)\n",
        "\n",
        "\t\tprint('Batch %d... (Done %.3f s)' % (batch_i, time.time() - prev_time))\n",
        "\t\tprev_time = time.time()\n",
        "\n",
        "\t# Bounding-box colors\n",
        "\tcolor_list = [[random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)] for _ in range(len(classes))]\n",
        "\n",
        "\tif len(img_detections) == 0:\n",
        "\t\treturn\n",
        "\n",
        "\t# Iterate through images and save plot of detections\n",
        "\tfor img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
        "\t\tprint(\"image %g: '%s'\" % (img_i, path))\n",
        "\n",
        "\t\tif opt.plot_flag:\n",
        "\t\t\timg = cv2.imread(path)\n",
        "\n",
        "\t\t# The amount of padding that was added\n",
        "\t\tpad_x = max(img.shape[0] - img.shape[1], 0) * (opt.img_size / max(img.shape))\n",
        "\t\tpad_y = max(img.shape[1] - img.shape[0], 0) * (opt.img_size / max(img.shape))\n",
        "\t\t# Image height and width after padding is removed\n",
        "\t\tunpad_h = opt.img_size - pad_y\n",
        "\t\tunpad_w = opt.img_size - pad_x\n",
        "\n",
        "\t\t# Draw bounding boxes and labels of detections\n",
        "\t\tif detections is not None:\n",
        "\t\t\tunique_classes = detections[:, -1].cpu().unique()\n",
        "\t\t\tbbox_colors = random.sample(color_list, len(unique_classes))\n",
        "\n",
        "\t\t\t# write results to .txt file\n",
        "\t\t\tresults_img_path = os.path.join(opt.output_folder, path.split('/')[-1])\n",
        "\t\t\tresults_txt_path = results_img_path + '.txt'\n",
        "\t\t\tif os.path.isfile(results_txt_path):\n",
        "\t\t\t\tos.remove(results_txt_path)\n",
        "\n",
        "\t\t\tfor i in unique_classes:\n",
        "\t\t\t\tn = (detections[:, -1].cpu() == i).sum()\n",
        "\t\t\t\tprint('%g %ss' % (n, classes[int(i)]))\n",
        "\n",
        "\t\t\tfor P1_x, P1_y, P2_x, P2_y, P3_x, P3_y, P4_x, P4_y, conf, cls_conf, cls_pred in detections:\n",
        "\t\t\t\tP1_y = max((((P1_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
        "\t\t\t\tP1_x = max((((P1_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
        "\t\t\t\tP2_y = max((((P2_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
        "\t\t\t\tP2_x = max((((P2_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
        "\t\t\t\tP3_y = max((((P3_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
        "\t\t\t\tP3_x = max((((P3_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
        "\t\t\t\tP4_y = max((((P4_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
        "\t\t\t\tP4_x = max((((P4_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
        "\n",
        "\t\t\t\t# write to file\n",
        "\t\t\t\tif opt.txt_out:\n",
        "\t\t\t\t\twith open(results_txt_path, 'a') as file:\n",
        "\t\t\t\t\t\tfile.write(('%g %g %g %g %g %g %g %g %g %g \\n') % (P1_x, P1_y, P2_x, P2_y, P3_x, P3_y, P4_x, P4_y, cls_pred, cls_conf * conf))\n",
        "\t\t\t\t\n",
        "\t\t\t\tif opt.plot_flag:\n",
        "\t\t\t\t\t# Add the bbox to the plot\n",
        "\t\t\t\t\tlabel = '%s %.2f' % (classes[int(cls_pred)], conf)\n",
        "\t\t\t\t\tcolor = bbox_colors[int(np.where(unique_classes == int(cls_pred))[0])]\n",
        "\t\t\t\t\tplot_one_box([P1_x, P1_y, P2_x, P2_y, P3_x, P3_y, P4_x, P4_y], img, label=None, color=color)\n",
        "\n",
        "\t\t\tcv2.imshow(path.split('/')[-1], img)\n",
        "\t\t\tcv2.waitKey(0)\n",
        "\t\t\tcv2.destroyAllWindows()\n",
        "\n",
        "\t\tif opt.plot_flag:\n",
        "\t\t\t# Save generated image with detections\n",
        "\t\t\tcv2.imwrite(results_img_path.replace('.bmp', '.jpg').replace('.tif', '.jpg'), img)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\ttorch.cuda.empty_cache()\n",
        "\tdetect(opt)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}